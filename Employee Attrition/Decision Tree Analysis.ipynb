{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Analysis\n",
    "\n",
    "In this Notebook file, I built a function to calculate entropy of a dataframe w.r.to a target column and use it to start building a decision tree for the employees dataset. \n",
    "1. You will see the successful identification of the features that provides MAX INFORMATION GAIN, be them continuous or discrete. \n",
    "2. I wrote two functions for these two types of features to identify the Info Gain and in the case of continuous variables the value that splits the variable to give the best information gain.\n",
    "3. Later on we build the tree manually using these two till depth 2 as a demonstration of successful application of fundamental concepts.\n",
    "\n",
    "Finally, we will use inbuilt features of scikit library to build the whole tree, whilst proving our earlier identifications and completing the tree.\n",
    "\n",
    "Additionally, we have an orange file that contains the same decision tree that serves as an additional validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>left</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>Division</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.88</td>\n",
       "      <td>7</td>\n",
       "      <td>272</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  satisfaction_level  last_evaluation  number_project  \\\n",
       "0   1                0.38             0.53               2   \n",
       "1   2                0.80             0.86               5   \n",
       "2   3                0.11             0.88               7   \n",
       "3   4                0.72             0.87               5   \n",
       "4   5                0.37             0.52               2   \n",
       "\n",
       "   average_montly_hours  time_spend_company  Work_accident  left  \\\n",
       "0                   157                   3              0     1   \n",
       "1                   262                   6              0     1   \n",
       "2                   272                   4              0     1   \n",
       "3                   223                   5              0     1   \n",
       "4                   159                   3              0     1   \n",
       "\n",
       "   promotion_last_5years Division  salary  \n",
       "0                      0    sales     low  \n",
       "1                      0    sales  medium  \n",
       "2                      0    sales  medium  \n",
       "3                      0    sales     low  \n",
       "4                      0    sales     low  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv(\"Why are employees leaving.csv\")  #Loading file into my Notebook\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sales', 'accounting', 'hr', 'technical', 'support', 'management',\n",
       "       'IT', 'product_mng', 'marketing', 'RandD'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Division'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Entropy for a dataset when given a target column ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate entropy based on the 'left' column\n",
    "import math\n",
    "\n",
    "def calculate_entropy(data, target_column):\n",
    "    \"\"\"\n",
    "    Calculate entropy for a given target column\n",
    "    Entropy = -Î£(p * log2(p)) where p is the probability of each class\n",
    "    \"\"\"\n",
    "    # Get value counts and probabilities\n",
    "    value_counts = data[target_column].value_counts()\n",
    "    total_samples = len(data)\n",
    "    \n",
    "    entropy = 0\n",
    "    # print(f\"Distribution of '{target_column}' column:\")\n",
    "    # print(f\"Total samples: {total_samples}\")\n",
    "    \n",
    "    for value, count in value_counts.items():\n",
    "        probability = count / total_samples\n",
    "        if probability > 0:  # Avoid log(0)\n",
    "            entropy -= probability * math.log2(probability)\n",
    "        # print(f\"  {target_column}={value}: {count} samples ({probability:.4f} probability)\")\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "# Calculate entropy for the 'left' column\n",
    "# entropy_left = calculate_entropy(df, 'left')\n",
    "# print(f\"\\n Baseline Entropy of the dataset based on 'left' column: {entropy_left:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gains for Discrete and Continuous Features ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFORMATION GAIN CALCULATION FOR DISCRETE FEATURES\n",
    "\n",
    "def calculate_information_gain_discrete(data, feature_column, target_column):\n",
    "    # Step 1: Calculate original entropy (baseline entropy of target variable)\n",
    "    # print(f\"=== INFORMATION GAIN ANALYSIS FOR '{feature_column.upper()}' COLUMN ===\\n\")\n",
    "    \n",
    "    original_entropy = calculate_entropy(data, target_column)\n",
    "    # print(f\"\\nBASELINE ENTROPY of '{target_column}' column: {original_entropy:.4f}\")\n",
    "    \n",
    "    # Step 2: Get unique values in the feature column to split on\n",
    "    unique_values = data[feature_column].unique()\n",
    "    # print(f\"\\nUNIQUE VALUES in '{feature_column}' column: {unique_values}\")\n",
    "    \n",
    "    total_samples = len(data)\n",
    "    weighted_entropy = 0\n",
    "    \n",
    "    for value in unique_values:\n",
    "        # Create subset of data for this feature value\n",
    "        subset = data[data[feature_column] == value]\n",
    "        subset_size = len(subset)\n",
    "        \n",
    "        # Calculate entropy for this subset\n",
    "        subset_entropy = calculate_entropy(subset, target_column)\n",
    "        \n",
    "        # Calculate weight (proportion of total data this subset represents)\n",
    "        weight = subset_size / total_samples\n",
    "        \n",
    "        # Add to weighted entropy calculation\n",
    "        weighted_entropy += weight * subset_entropy\n",
    "    \n",
    "    # Step 4: Calculate Information Gain\n",
    "    information_gain = original_entropy - weighted_entropy\n",
    "   \n",
    "    # print(f\"\\nInformation Gain is {information_gain:.4f} for {feature_column} column\")\n",
    "    return {'information_gain': round(information_gain, 4),\n",
    "            'weighted_entropy': round(weighted_entropy, 4), \n",
    "            'original_entropy': round(original_entropy, 4),\n",
    "            'feature_column': feature_column,\n",
    "            'split': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information Gain for Continuous Features\n",
    "\n",
    "def calculate_information_gain_continuous(data, feature_column, target_column):\n",
    "    # Check if data is empty\n",
    "    if len(data) == 0:\n",
    "        print(f\"Warning: No data available for feature '{feature_column}'\")\n",
    "        return {'information_gain': 0, \n",
    "                'weighted_entropy': 0, \n",
    "                'original_entropy': 0, \n",
    "                'feature_column': feature_column, \n",
    "                'split': None}\n",
    "    \n",
    "    # Sort the unique values of the feature column\n",
    "    original_entropy = calculate_entropy(data, target_column)\n",
    "    # print(f\"\\nBASELINE ENTROPY of '{target_column}' column: {original_entropy:.4f}\")\n",
    "    total_samples = len(data)\n",
    "\n",
    "    # print(f\"{feature_column}\")\n",
    "    \n",
    "    sorted_values = np.sort(data[feature_column].unique())\n",
    "    # print(\"Sorted values:\", sorted_values, len(sorted_values))\n",
    "    \n",
    "    # Check if there are no unique values or only one unique value\n",
    "    if len(sorted_values) < 1:\n",
    "        print(f\"Warning: Cannot create splits for feature '{feature_column}' - insufficient unique values\")\n",
    "        return {'information_gain': 0, \n",
    "                'weighted_entropy': original_entropy, \n",
    "                'original_entropy': round(original_entropy, 4), \n",
    "                'feature_column': feature_column, \n",
    "                'split': sorted_values[0] if len(sorted_values) == 1 else None}\n",
    "\n",
    "\n",
    "    def create_feature_splits(sorted_values):\n",
    "        modified_sortedValues = []\n",
    "        for i in range(len(sorted_values) - 1):\n",
    "            modified_sortedValues.append((sorted_values[i] + sorted_values[i+1])/2)\n",
    "        return modified_sortedValues\n",
    "    \n",
    "    feature_splits = create_feature_splits(sorted_values)\n",
    "    # print(\"Feature splits:\", feature_splits, len(feature_splits))\n",
    "    \n",
    "    # Check if feature_splits is empty\n",
    "    if len(feature_splits) == 0:\n",
    "        print(f\"Warning: No splits possible for feature '{feature_column}'\")\n",
    "        return {'information_gain': 0, \n",
    "                'weighted_entropy': original_entropy, \n",
    "                'original_entropy': round(original_entropy, 4), \n",
    "                'feature_column': feature_column, \n",
    "                'split': None}\n",
    "    \n",
    "    selected_split = feature_splits[0]\n",
    "    max_information_gain = 0\n",
    "    max_weighted_entropy = original_entropy\n",
    "    \n",
    "    # Calculate the information gain for each range\n",
    "    for i,_ in enumerate(feature_splits):\n",
    "        # print(f\"Split {i}: {feature_splits[i], sorted_values[i], sorted_values[i+1]}\")\n",
    "        subset1 = data[data[feature_column] >= feature_splits[i]]\n",
    "        subset1_size = len(subset1)\n",
    "        subset1_entropy = calculate_entropy(subset1, target_column)\n",
    "        weight1 = subset1_size / total_samples\n",
    "        # print(\"Subset:\", len(subset))\n",
    "        \n",
    "        subset2 = data[data[feature_column] < feature_splits[i]]\n",
    "        subset2_size = len(subset2)\n",
    "        subset2_entropy = calculate_entropy(subset2, target_column)\n",
    "        weight2 = subset2_size / total_samples\n",
    "\n",
    "        # print(f\"Split {i}: {feature_splits[i]}\")\n",
    "        # print(f\"Subset1: {subset1_size} samples, entropy: {subset1_entropy:.4f}\")\n",
    "        # print(f\"Subset2: {subset2_size} samples, entropy: {subset2_entropy:.4f}\")\n",
    "\n",
    "        weighted_entropy = weight1 * subset1_entropy + weight2 * subset2_entropy\n",
    "        information_gain = original_entropy - weighted_entropy\n",
    "\n",
    "        if information_gain > max_information_gain:\n",
    "            max_information_gain = information_gain\n",
    "            selected_split = feature_splits[i]\n",
    "            max_weighted_entropy = weighted_entropy\n",
    "        # print(\"Subset1:\", len(subset1))\n",
    "    # print(f\"\\nMax information gain is {max_information_gain:.4f} for {feature_column} column and split is {selected_split}, {max_weighted_entropy}\")\n",
    "    return {'information_gain': round(max_information_gain, 4), \n",
    "            'weighted_entropy': round(max_weighted_entropy, 4), \n",
    "            'original_entropy': round(original_entropy, 4), \n",
    "            'feature_column': feature_column, \n",
    "            'split': round(selected_split, 3) if selected_split is not None else None}\n",
    "\n",
    "# calculate_information_gain_continuous(df, 'satisfaction_level', 'left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the above two functions to calculate max info gain across the whole dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.describe()\n",
    "def calculate_max_information_gain(df, df_name):\n",
    "    satisfaction= calculate_information_gain_continuous(df, 'satisfaction_level', 'left')\n",
    "    last_evaluation= calculate_information_gain_continuous(df, 'last_evaluation', 'left')\n",
    "    salary= calculate_information_gain_discrete(df, 'salary', 'left')\n",
    "    division= calculate_information_gain_discrete(df, 'Division', 'left')\n",
    "    average_montly_hours= calculate_information_gain_continuous(df, 'average_montly_hours', 'left')\n",
    "    time_spend_company= calculate_information_gain_continuous(df, 'time_spend_company', 'left')\n",
    "    number_project= calculate_information_gain_continuous(df, 'number_project', 'left')\n",
    "    work_accident= calculate_information_gain_discrete(df, 'Work_accident', 'left')\n",
    "    promotion_last_5years= calculate_information_gain_discrete(df, 'promotion_last_5years', 'left')\n",
    "    \n",
    "    features = [satisfaction, last_evaluation, salary, division, average_montly_hours, time_spend_company, number_project, work_accident, promotion_last_5years]\n",
    "\n",
    "    information_gains = [feature['information_gain'] for feature in features]\n",
    "    max_information_gain = max(information_gains)\n",
    "    # print(f\"\\nOptimal Feature information for {df_name} \\n {features[information_gains.index(max_information_gain)]}\")\n",
    "    return features[information_gains.index(max_information_gain)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of the above on the Dataset ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Apply Function on the Root Node (Current Depth 1) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimum Feature for Root Node is {'information_gain': 0.1926, 'weighted_entropy': 0.5993, 'original_entropy': 0.7918, 'feature_column': 'satisfaction_level', 'split': 0.465}\n"
     ]
    }
   ],
   "source": [
    "optimum_feature = calculate_max_information_gain(df, \"Root Node\")\n",
    "print(f\"\\nOptimum Feature for Root Node is {optimum_feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 2 - Split Root Node and apply the function on each leaf (Depth 2) ####\n",
    "We split the data of root node into two based on the suggested split in the above output i.e Satisfaction level and 0.465\n",
    "1. Node 1 for Sat. Level <= 0.465\n",
    "2. NODE 2 for Sat. Level > 0.465"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimum Feature for NODE 1 is {'information_gain': 0.1802, 'weighted_entropy': 0.7877, 'original_entropy': 0.9679, 'feature_column': 'satisfaction_level', 'split': 0.115}\n",
      "\n",
      " Optimum Feature for NODE 2 is {'information_gain': 0.185, 'weighted_entropy': 0.2716, 'original_entropy': 0.4567, 'feature_column': 'time_spend_company', 'split': 4.5}\n"
     ]
    }
   ],
   "source": [
    "#NODE 1 => Sat. Level <= 0.465\n",
    "df1 = df[df['satisfaction_level'] <= 0.465]\n",
    "optimum_feature_1 = calculate_max_information_gain(df1, \"NODE 1\")\n",
    "\n",
    "#NODE 2 => Sat. Level > 0.465\n",
    "df2 = df[df['satisfaction_level'] > 0.465]\n",
    "optimum_feature_2 = calculate_max_information_gain(df2, \"NODE 2\")\n",
    "\n",
    "print (f\"\\n Optimum Feature for NODE 1 is {optimum_feature_1}\")\n",
    "print (f\"\\n Optimum Feature for NODE 2 is {optimum_feature_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 3 - Split Nodes 1 and 2 to grow the tree further (Depth 3) ####\n",
    "1. NODE 1 is going to be split into NODE 1.1 and NODE 1.2 based on satisfaction level at the value 0.115.\n",
    "2. NODE 2 is going to be split into NODE 2.1 and NODE 2.2 based on time spent at the compnay at the value 4.5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimum Feature for NODE 1.1 is {'information_gain': 0, 'weighted_entropy': 0.0, 'original_entropy': 0.0, 'feature_column': 'satisfaction_level', 'split': 0.095}\n",
      "\n",
      " Optimum Feature for NODE 1.2 is {'information_gain': 0.5379, 'weighted_entropy': 0.4621, 'original_entropy': 1.0, 'feature_column': 'number_project', 'split': 2.5}\n",
      "\n",
      " Optimum Feature for NODE 2.1 is {'information_gain': 0.0063, 'weighted_entropy': 0.103, 'original_entropy': 0.1093, 'feature_column': 'average_montly_hours', 'split': 290.5}\n",
      "\n",
      " Optimum Feature for NODE 2.2 is {'information_gain': 0.3694, 'weighted_entropy': 0.626, 'original_entropy': 0.9954, 'feature_column': 'last_evaluation', 'split': 0.805}\n"
     ]
    }
   ],
   "source": [
    "df11 = df1[df1['satisfaction_level'] <= 0.115]\n",
    "optimum_feature_11 = calculate_max_information_gain(df11, \"NODE 1.1\")\n",
    "\n",
    "df12 = df1[df1['satisfaction_level'] > 0.115]\n",
    "optimum_feature_12 = calculate_max_information_gain(df12, \"NODE 1.2\")\n",
    "\n",
    "print (f\"\\n Optimum Feature for NODE 1.1 is {optimum_feature_11}\")\n",
    "print (f\"\\n Optimum Feature for NODE 1.2 is {optimum_feature_12}\")\n",
    "\n",
    "\n",
    "#----------------------------------\n",
    "\n",
    "df21 = df2[df2['time_spend_company'] <= 4.5]\n",
    "optimum_feature_21 = calculate_max_information_gain(df21, \"NODE 2.1\")\n",
    "\n",
    "df22 = df2[df2['time_spend_company'] > 4.5]\n",
    "optimum_feature_22 = calculate_max_information_gain(df22, \"NODE 2.2\")\n",
    "\n",
    "print (f\"\\n Optimum Feature for NODE 2.1 is {optimum_feature_21}\")\n",
    "print (f\"\\n Optimum Feature for NODE 2.2 is {optimum_feature_22}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping points and further steps ###\n",
    "1. Node 1.1 is already completely sorted as the entropy is 0. No need to further prune it.\n",
    "2. Node 2.1 has the max information gain to be just 0.0063, further building the tree from this node will not result in much value or information.\n",
    "\n",
    "Stopping criteria have to be clearly defined and the tree can be grown till we run out of features or all terminal nodes hit the stopping criteria and we have a fully grown decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scikit Learn to build and grow the same decision tree ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14999 entries, 0 to 14998\n",
      "Data columns (total 11 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   ID                     14999 non-null  int64  \n",
      " 1   satisfaction_level     14999 non-null  float64\n",
      " 2   last_evaluation        14999 non-null  float64\n",
      " 3   number_project         14999 non-null  int64  \n",
      " 4   average_montly_hours   14999 non-null  int64  \n",
      " 5   time_spend_company     14999 non-null  int64  \n",
      " 6   Work_accident          14999 non-null  int64  \n",
      " 7   left                   14999 non-null  int64  \n",
      " 8   promotion_last_5years  14999 non-null  int64  \n",
      " 9   Division               14999 non-null  object \n",
      " 10  salary                 14999 non-null  object \n",
      "dtypes: float64(2), int64(7), object(2)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that 2 out of the 10 features (excluding ID) of this dataset are categorical, which will have to be converted into numerical ones to be used for modeling. For this, we can make use of __LabelEncoder__ from the Scikit-Learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14999 entries, 0 to 14998\n",
      "Data columns (total 11 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   ID                     14999 non-null  int64  \n",
      " 1   satisfaction_level     14999 non-null  float64\n",
      " 2   last_evaluation        14999 non-null  float64\n",
      " 3   number_project         14999 non-null  int64  \n",
      " 4   average_montly_hours   14999 non-null  int64  \n",
      " 5   time_spend_company     14999 non-null  int64  \n",
      " 6   Work_accident          14999 non-null  int64  \n",
      " 7   left                   14999 non-null  int64  \n",
      " 8   promotion_last_5years  14999 non-null  int64  \n",
      " 9   Division               14999 non-null  int64  \n",
      " 10  salary                 14999 non-null  int64  \n",
      "dtypes: float64(2), int64(9)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "# Create a copy of the original dataframe\n",
    "df_encoded = df.copy()\n",
    "\n",
    "division_encoder = LabelEncoder()\n",
    "salary_encoder = LabelEncoder()\n",
    "df_encoded['Division'] = division_encoder.fit_transform(df_encoded['Division'])\n",
    "df_encoded['salary'] = salary_encoder.fit_transform(df_encoded['salary'])\n",
    "df_encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'IT': 0, 'RandD': 1, 'accounting': 2, 'hr': 3, 'management': 4, 'marketing': 5, 'product_mng': 6, 'sales': 7, 'support': 8, 'technical': 9}\n",
      "{'high': 0, 'low': 1, 'medium': 2}\n"
     ]
    }
   ],
   "source": [
    "# The Encodings are done as follows:\n",
    "\n",
    "print (dict(zip(division_encoder.classes_, division_encoder.transform(division_encoder.classes_))))\n",
    "print (dict(zip(salary_encoder.classes_, salary_encoder.transform(salary_encoder.classes_))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
